{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter('ignore')\n",
    "import pandas as pd, numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "from unicodedata import normalize\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jairbolsonaro.json', 'r', encoding='utf8') as json_file:\n",
    "    words = json.load(json_file)\n",
    "    \n",
    "columns_name = []\n",
    "for i in words[0]:\n",
    "    columns_name.append(i)\n",
    "\n",
    "DF = pd.DataFrame(columns=columns_name)\n",
    "DF = DF[['full_text', 'created_at', 'source', \n",
    "         #'geo', 'coordinates', \n",
    "         'place', 'retweet_count']]\n",
    "    \n",
    "\n",
    "DF['full_text'] = [i['full_text']  for i in words]\n",
    "DF['created_at'] = [i['created_at'] for i in words]\n",
    "DF['entities'] = [i['entities'] for i in words]\n",
    "DF['source'] = [i['source'] for i in words]\n",
    "#DF['geo'] = [i['geo'] for i in words]\n",
    "#DF['coordinates'] = [i['coordinates'] for i in words]\n",
    "DF['place'] = [i['place'] for i in words]\n",
    "DF['retweet_count'] = [i['retweet_count'] for i in words]\n",
    "DF['day'] = [str(i)[8:10].strip() for i in pd.to_datetime(DF['created_at'])]\n",
    "DF['month'] = [str(i)[5:7].strip() for i in pd.to_datetime(DF['created_at'])]\n",
    "DF['year'] = [str(i)[0:4].strip() for i in pd.to_datetime(DF['created_at'])]\n",
    "DF['hour'] = [str(i)[10:19].strip() for i in pd.to_datetime(DF['created_at'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retorna o texto limpo em forma de lista\n",
    "def return_text_clean_split(DF):\n",
    "    words_list = []\n",
    "    stopWords = stopwords.words('portuguese')\n",
    "    list_alphabetic = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', \n",
    "                       'i', 'j', 'l', 'm', 'n', 'o', 'p', 'q', \n",
    "                       'r', 's', 't', 'u', 'v', 'x', 'z']\n",
    "    for i in list_alphabetic:\n",
    "        stopWords.append(i)\n",
    "        \n",
    "    for i in DF:\n",
    "        word_tokens = word_tokenize(str(i))\n",
    "        for word in word_tokens:\n",
    "            \n",
    "            new_words = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word, flags=re.MULTILINE).replace('http', '').replace('https', '') #Remove links\n",
    "            new_words = re.sub(r'[^\\w]', '', new_words) #Remove espaços _ , () / \n",
    "            if new_words.lower().strip() not in stopWords:  #Remove palavras stopWords\n",
    "                words_list.append(new_words.lower())\n",
    "                \n",
    "    #phrase = \" \".join(s for s in words_list)\n",
    "    return words_list\n",
    "\n",
    "\n",
    "\n",
    "#retorna o texto limpo em forma de string\n",
    "def return_text_clean(text, stop_words=True):\n",
    "    words_list = []\n",
    "    stopWords = stopwords.words('portuguese')\n",
    "    list_alphabetic = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', \n",
    "                       'i', 'j', 'l', 'm', 'n', 'o', 'p', 'q', \n",
    "                       'r', 's', 't', 'u', 'v', 'x', 'z']\n",
    "    for i in list_alphabetic:\n",
    "        stopWords.append(i)\n",
    "        \n",
    "    \n",
    "    word_tokens = word_tokenize(str(text))\n",
    "    for word in word_tokens:\n",
    "        \n",
    "        \n",
    "        new_words = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', word, flags=re.MULTILINE).replace('http', '').replace('https', '') #Remove links\n",
    "        new_words = re.sub(r'[^\\w]', '', new_words) #Remove espaços _ , () / \n",
    "        new_words = re.sub('[^a-zA-Z]+', ' ', (new_words.lower())) #Deixa apenas letras e remove os numeros\n",
    "        \n",
    "        if stop_words:\n",
    "            if new_words.lower().strip() not in stopWords:  #Remove palavras stopWords\n",
    "                words_list.append(new_words.lower())\n",
    "        else:\n",
    "            \n",
    "            if not new_words.lower().strip() in list_alphabetic: \n",
    "                words_list.append(new_words.lower().strip())\n",
    "            \n",
    "                \n",
    "    full_text = \" \".join(s for s in words_list)\n",
    "    return full_text\n",
    "\n",
    "\n",
    "# Transforma as horas em intervalos.\n",
    "def get_range_hour(hour):\n",
    "    \n",
    "    hour = datetime.strptime(hour, '%H:%M:%S').time()\n",
    "    return_range = None\n",
    "    \n",
    "    #Manhã\n",
    "    time_str_morning_begin = '06:00:00'\n",
    "    time_str_morning_end = '11:59:59'\n",
    "\n",
    "    time_morning_begin = datetime.strptime(time_str_morning_begin, '%H:%M:%S').time()\n",
    "    time_morning_end = datetime.strptime(time_str_morning_end, '%H:%M:%S').time()\n",
    "\n",
    "    #Tarde \n",
    "    time_str_evening_begin = '12:00:00'\n",
    "    time_str_evening_end = '17:59:59'\n",
    "\n",
    "    time_evening_begin = datetime.strptime(time_str_evening_begin, '%H:%M:%S').time()\n",
    "    time_evening_end = datetime.strptime(time_str_evening_end, '%H:%M:%S').time()\n",
    "\n",
    "    #Noite \n",
    "    time_str_night_begin = '18:00:00'\n",
    "    time_str_night_end = '23:59:59'\n",
    "\n",
    "    time_night_begin = datetime.strptime(time_str_night_begin, '%H:%M:%S').time()\n",
    "    time_night_end = datetime.strptime(time_str_night_end, '%H:%M:%S').time()\n",
    "\n",
    "    #Madrugada\n",
    "    time_str_dawn_begin = '00:00:00'\n",
    "    time_str_dawn_end = '05:59:59'\n",
    "\n",
    "    time_dawn_begin = datetime.strptime(time_str_dawn_begin, '%H:%M:%S').time()\n",
    "    time_dawn_end = datetime.strptime(time_str_dawn_end, '%H:%M:%S').time()\n",
    "    \n",
    "    #Manha\n",
    "    if hour >= time_morning_begin and hour <= time_morning_end:\n",
    "        return_range = 1 \n",
    "\n",
    "    #Tarde    \n",
    "    elif hour >= time_evening_begin and hour <= time_evening_end:\n",
    "        return_range = 2 \n",
    "        \n",
    "    #Noite    \n",
    "    elif hour >= time_night_begin and hour <= time_night_end:\n",
    "        return_range = 3 \n",
    "\n",
    "    #Madrugada    \n",
    "    elif hour >= time_dawn_begin and hour <= time_dawn_end:\n",
    "        return_range = 4\n",
    "    \n",
    "    return return_range\n",
    "\n",
    "\n",
    "\n",
    "#Retorna a frequencia das palavras\n",
    "def freq(DF, qtd):\n",
    "    counts = Counter(return_text_clean_split(DF))\n",
    "    counts.pop('') #remove os vazios\n",
    "    return_counts = counts.most_common(qtd) #Top 10\n",
    "    \n",
    "    for i, j in return_counts:\n",
    "        print('Palavras :'+i, '  Frequência :'+str(j))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "#Gera as hashTags mais usadas no dia.     \n",
    "def generate_hashtags_per_day(DF, show_qtd):\n",
    "    \n",
    "    hashtaglist_morning = []\n",
    "    hashtaglist_evening = []\n",
    "    hashtaglist_night = []\n",
    "    hashtaglist_dawn = []\n",
    "    \n",
    "    \n",
    "\n",
    "    for row in range(DF.shape[0]):\n",
    "        hour = DF['hour'][row]\n",
    "        full_text = DF['full_text'][row].split()\n",
    "        \n",
    "        for words in full_text:\n",
    "            words = words.lower()\n",
    "            #Considera uma hashtag se a palavra se iniciar com #\n",
    "            if words.lower().startswith('#'):\n",
    "                #Manhã\n",
    "                if get_range_hour(hour) == 1:\n",
    "                    hashtaglist_morning.append(words)\n",
    "\n",
    "                #Tarde    \n",
    "                elif get_range_hour(hour) == 2:\n",
    "                    hashtaglist_evening.append(words)\n",
    "\n",
    "                #Noite    \n",
    "                elif get_range_hour(hour) == 3:\n",
    "                    hashtaglist_night.append(words)\n",
    "\n",
    "                #Madrugada    \n",
    "                elif get_range_hour(hour) == 4:\n",
    "                    hashtaglist_dawn.append(words)\n",
    "\n",
    "    df_morning = pd.DataFrame(columns=['turno', 'hashtags', 'freq'])\n",
    "    df_evening = pd.DataFrame(columns=['turno', 'hashtags', 'freq'])\n",
    "    df_night = pd.DataFrame(columns=['turno', 'hashtags', 'freq'])\n",
    "    df_dawn = pd.DataFrame(columns=['turno', 'hashtags', 'freq'])\n",
    "    \n",
    "       \n",
    "    \n",
    "    df_morning['hashtags'] = [i[0] for i in Counter(hashtaglist_morning).most_common(show_qtd)]\n",
    "    df_morning['freq'] = [i[1] for i in Counter(hashtaglist_morning).most_common(show_qtd)]\n",
    "    df_morning['turno'] = 'manha'\n",
    "    \n",
    "    df_evening['hashtags'] = [i[0] for i in Counter(hashtaglist_evening).most_common(show_qtd)]\n",
    "    df_evening['freq'] = [i[1] for i in Counter(hashtaglist_evening).most_common(show_qtd)]\n",
    "    df_evening['turno'] = 'tarde'\n",
    "    \n",
    "    df_night['hashtags'] = [i[0] for i in Counter(hashtaglist_night).most_common(show_qtd)]\n",
    "    df_night['freq'] = [i[1] for i in Counter(hashtaglist_night).most_common(show_qtd)]\n",
    "    df_night['turno'] = 'noite'\n",
    "    \n",
    "    df_dawn['hashtags'] = [i[0] for i in Counter(hashtaglist_dawn).most_common(show_qtd)]\n",
    "    df_dawn['freq'] = [i[1] for i in Counter(hashtaglist_dawn).most_common(show_qtd)]\n",
    "    df_dawn['turno'] = 'madrugada'\n",
    "    \n",
    "   \n",
    "    df_union = pd.concat([df_morning,\n",
    "                            df_evening,\n",
    "                            df_night ,\n",
    "                            df_dawn ])\n",
    "    \n",
    "    \n",
    "    return df_union #Counter(hashtaglist_morning).most_common(show_qtd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def untokenize(ngram):\n",
    "    tokens = list(ngram)\n",
    "    return \"\".join([\" \"+i if not i.startswith(\"'\") and \\\n",
    "                             i not in string.punctuation and \\\n",
    "                             i != \"n't\"\n",
    "                          else i for i in tokens]).strip()\n",
    "\n",
    "\n",
    "def generate_sentences(DF, length_sentence, string_search):\n",
    "    #Negrito.\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'\n",
    "    only_valid_words = re.compile('[A-Za-z]+: (.*)') #Apenas palavras válidas\n",
    "    w_list = []\n",
    "    sentence_list = []\n",
    "      \n",
    "    for row in range(DF.shape[0]):\n",
    "        \n",
    "        text = DF['full_text'][row].split()\n",
    "        text_list = []\n",
    "        \n",
    "            \n",
    "        for w in text:\n",
    "            w = return_text_clean(w, False)\n",
    "            text_list.append(w)\n",
    "            \n",
    "        str_text = ' '.join(text_list)\n",
    "        \n",
    "        #Deixa a frase de maneira natural, sem acentos e busca a string informada\n",
    "        if len(re.findall(string_search, normalize('NFKD', str_text).encode('ASCII','ignore').decode('ASCII'))):\n",
    "            w_list.append(str_text)\n",
    "\n",
    "    for word in w_list:\n",
    "        for sent in nltk.sent_tokenize(word):\n",
    "            strip_speaker = only_valid_words.match(sent)\n",
    "            if strip_speaker is not None:\n",
    "                sent = strip_speaker.group(1)\n",
    "            words = nltk.word_tokenize(sent)\n",
    "            for phrase in ngrams(words, length_sentence):\n",
    "                if all(word not in string.punctuation for word in phrase): #Remove pontuações\n",
    "                    #phrase = re.sub(r'[^A-Za-z0-9]+', ' ', untokenize(phrase)) #apenas palavras válidas\n",
    "                    phrase = untokenize(phrase) #apenas palavras válidas\n",
    "                    phrase = phrase.replace('.', '').replace(\"''\", '')\n",
    "                    if phrase not in '  ':\n",
    "                        sentence_list.append(phrase)\n",
    "\n",
    "    phrase_counter = Counter(sentence_list).most_common(20)\n",
    "\n",
    "    for k,v in phrase_counter:\n",
    "        print(f'Frequencia: {BOLD}{str(v)}{END} vezes', \n",
    "              f'   Tamanho sequencia: {BOLD}{str(length_sentence)}{END} palavras', \n",
    "              f'   Sentenças: {BOLD}{k}{END}')\n",
    "        \n",
    "        \n",
    "#Faz a contagem de todos  dispositivos onde foram feito os posts\n",
    "def count_post_devices(DF):\n",
    "    iphone_list = []\n",
    "    android_list = []\n",
    "    others_list = []\n",
    "    \n",
    "    for row in range(DF.shape[0]):\n",
    "\n",
    "        row = DF['source'][row].lower()\n",
    "\n",
    "        row = row.split()[-1].replace('</a>', '').replace('rel=\"nofollow\">', '')\n",
    "\n",
    "        if row == 'iphone':\n",
    "            iphone_list.append(row)\n",
    "        elif row == 'android':\n",
    "            android_list.append(row)\n",
    "        else:\n",
    "            others_list.append('others')\n",
    "\n",
    "    #Obtem a contagem das listas de cada devices\n",
    "    iphone = Counter(iphone_list)\n",
    "    android = Counter(android_list)\n",
    "    others = Counter(others_list)\n",
    "    \n",
    "       \n",
    "    \n",
    "    list_count = [iphone['iphone'], android['android'], others['others']]\n",
    "    \n",
    "    \n",
    "    d = {'list_devices': ['iphone', 'android', 'others'], \n",
    "        'count': list_count,\n",
    "        'percentage': [(round(i / np.sum(list_count), 2) * 100) for i in list_count]}\n",
    "    \n",
    "    \n",
    "    df_return = pd.DataFrame(d)\n",
    "    return df_return\n",
    "\n",
    "\n",
    "#Faz a plotagem da contagem de dispositivos onde foram feitos os posts\n",
    "def plot_count_posts_devices(DF):    \n",
    "    x = count_post_devices(DF)[['list_devices', 'percentage']]\n",
    "    colors= ['blue', 'red', 'green']\n",
    "    f, ax = plt.subplots(figsize=(18,5))\n",
    "    \n",
    "    ax.bar(x['list_devices'], x['percentage'], color=colors, label=colors)\n",
    "    plt.xticks(fontsize = 11, rotation=45)\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xlabel('list_devices')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Faz a contagem dos dispositivos onde o usuário twitou usando algo relacionado a bolsonaro, lula e dilma\n",
    "def count_post_devices_per_candidate(DF):\n",
    "    \n",
    "    candidates = ['bolsonaro', 'lula', 'dilma']\n",
    "\n",
    "    list_data = []\n",
    "\n",
    "    for candidate in candidates:\n",
    "        iphone_list = []\n",
    "        android_list = []\n",
    "        others_list = []\n",
    "        dict_count = {}\n",
    "\n",
    "        for row in range(DF.shape[0]):\n",
    "\n",
    "            if len(re.findall(candidate, normalize('NFKD', DF['full_text'][row].lower()).encode('ASCII','ignore').decode('ASCII'))):\n",
    "\n",
    "                row = DF['source'][row].lower()\n",
    "\n",
    "                row = row.split()[-1].replace('</a>', '').replace('rel=\"nofollow\">', '')\n",
    "\n",
    "                if row == 'iphone':\n",
    "                    iphone_list.append(row)\n",
    "                elif row == 'android':\n",
    "                    android_list.append(row)\n",
    "                else:\n",
    "                    others_list.append('others')\n",
    "\n",
    "        #Obtem a contagem das listas de cada devices\n",
    "        iphone = Counter(iphone_list)\n",
    "        android = Counter(android_list)\n",
    "        others = Counter(others_list)\n",
    "\n",
    "        dict_count['candidate'] = candidate\n",
    "        dict_count['iphone'] = iphone['iphone']\n",
    "        dict_count['android'] = android['android']\n",
    "        dict_count['others'] = others['others']\n",
    "\n",
    "\n",
    "        list_data.append(dict_count)\n",
    "    \n",
    "    #Cria dataframe para posteriormente ser retornado pela função\n",
    "    d = pd.DataFrame(list_data)\n",
    "\n",
    "    #Calcula a porcentagem de cada device\n",
    "    #Calculo da porcentagem: (device / (somatorio de cada devices)) * 100\n",
    "    iphone_percentage = [str(round(i, 2) * 100) + '%' for i in (d['iphone'] / (d['iphone'] + d['android'] + d['others'])).values]\n",
    "    android_percentage = [str(round(i, 2) * 100) + '%' for i in (d['android'] / (d['iphone'] + d['android'] + d['others'])).values]\n",
    "    others_percentage = [str(round(i, 2) * 100) + '%' for i in (d['others'] / (d['iphone'] + d['android'] + d['others'])).values]\n",
    "\n",
    "    #Anexa as porcentagems no dataframe\n",
    "    d['iphone_percengate'] = iphone_percentage\n",
    "    d['android_percentage'] = android_percentage\n",
    "    d['others_percentage'] = others_percentage\n",
    "    \n",
    "    \n",
    "    d1 = d[['candidate', 'iphone', 'android', 'others']]\n",
    "    d2 = d[['candidate', 'iphone_percengate', 'android_percentage', 'others_percentage']]\n",
    "    \n",
    "    #Transoforma linhas em colunas para linhas. (UNPIVOT)\n",
    "    d1 = d1.melt(id_vars=['candidate'], var_name='list_devices', value_name='count') \n",
    "    d2 = d2.melt(id_vars=['candidate'], var_name='list_percetage', value_name='percetage') \n",
    "\n",
    "    d1['percetage'] = d2['percetage']\n",
    "    \n",
    "    return d1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_posts_devices(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
